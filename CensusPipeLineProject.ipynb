{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Library Imports"
      ],
      "metadata": {
        "id": "KLN3ihlqF5Ny"
      },
      "id": "KLN3ihlqF5Ny"
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the Pandas, MongoDb, MySql Modules\n",
        "\n",
        "!pip install pymongo\n",
        "!pip install mysql-connector-python\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JCL-_pydvk3S"
      },
      "id": "JCL-_pydvk3S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e555a0-186f-4410-8fa3-9aebd254f3b6",
      "metadata": {
        "id": "43e555a0-186f-4410-8fa3-9aebd254f3b6"
      },
      "outputs": [],
      "source": [
        "#Import the libraries required for the project\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import mysql.connector\n",
        "from pymongo import MongoClient\n",
        "from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, ForeignKey\n",
        "import sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Mount the google drive to be able to read the CSV file\n",
        "  # Ensure the file to be read is uploaded in the current google drive folder path\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i3E7JCicn1jK"
      },
      "id": "i3E7JCicn1jK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas Dataframe Operations on the Dataset"
      ],
      "metadata": {
        "id": "WS8s1eBWGEBX"
      },
      "id": "WS8s1eBWGEBX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Excel file which has the Census Data to be processed into DataFrame (DF1)\n",
        "df1 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/census_2011.xlsx')\n",
        "display(df1)\n"
      ],
      "metadata": {
        "id": "JjTgCjVYn99H"
      },
      "id": "JjTgCjVYn99H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e6240d-95ef-463e-9289-bc961dd7f184",
      "metadata": {
        "id": "b2e6240d-95ef-463e-9289-bc961dd7f184"
      },
      "outputs": [],
      "source": [
        "# set option to display all the rows and columns of the dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5074469-e9b8-4554-89e0-41d2fcced658",
      "metadata": {
        "id": "e5074469-e9b8-4554-89e0-41d2fcced658"
      },
      "outputs": [],
      "source": [
        "# (Task 1 : Rename the Given Columns)\n",
        "df1.rename(columns = {'State name': 'State_UT'}, inplace = True)\n",
        "df1.rename(columns = {'District name': 'District'}, inplace = True)\n",
        "df1.rename(columns = {' Male_Literate ': 'Literate_Male'}, inplace = True)\n",
        "df1.rename(columns = { 'Female_Literate': 'Literate_Female'}, inplace = True)\n",
        "df1.rename(columns = {'Rural_Households': 'Households_Rural'}, inplace = True)\n",
        "df1.rename(columns = { 'Urban_ Households': 'Households_Urban'}, inplace = True)\n",
        "df1.rename(columns = { 'Age_Group_0_29': 'Young_and_Adult'}, inplace = True)\n",
        "df1.rename(columns = { 'Age_Group_30_49': 'Middle_Aged'}, inplace = True)\n",
        "df1.rename(columns = { 'Age_Group_50': 'Senior_Citizen'}, inplace = True)\n",
        "df1.rename(columns = { 'Age not stated': 'Age_Not_Stated'}, inplace = True)\n",
        "\n",
        "display(df1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the State Column in title case and change AND to and\n",
        "df1['State_UT'] = df1['State_UT'].str.title()\n",
        "df1[\"State_UT\"] = df1[\"State_UT\"].apply(lambda x: x.replace(\" And \", \" and \"))\n",
        "display(df1)"
      ],
      "metadata": {
        "id": "lKpzPQqw1GXp"
      },
      "id": "lKpzPQqw1GXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401cb94d-2c70-4486-83b5-8b1af966d7d9",
      "metadata": {
        "id": "401cb94d-2c70-4486-83b5-8b1af966d7d9"
      },
      "outputs": [],
      "source": [
        "#Update Telegana for listed Districts\n",
        "\n",
        "df_Telangana = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Telangana.txt', delimiter=\" \",header=None)\n",
        "df_Telangana.columns=[\"TelDist\"]\n",
        "TDist = df_Telangana.TelDist.tolist()\n",
        "for i in range (0, df1.shape[0]):\n",
        "    if df1.District[i] in TDist:\n",
        "        df1.loc[i,['State/UT']] = \"Telangana\"\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "df1.loc[df1['State/UT'] == 'Telangana', ['State/UT','District']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f382ee8c-52cd-4c03-bd33-81ee4ed0d433",
      "metadata": {
        "id": "f382ee8c-52cd-4c03-bd33-81ee4ed0d433"
      },
      "outputs": [],
      "source": [
        "#Update Leh Ladhak for listed Districts\n",
        "LadakhDist = ['Leh(Ladakh)','Kargil']\n",
        "for i in range (0, df1.shape[0]):\n",
        "    if df1.District[i] in LadakhDist:\n",
        "        df1.loc[i,['State/UT']] = \"Ladakh\"\n",
        "    else:\n",
        "        continue\n",
        "df1.loc[df1['State/UT'] == 'Ladakh', ['State/UT','District']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b01c43b-b2cb-4067-b881-de3bd0f176b2",
      "metadata": {
        "id": "9b01c43b-b2cb-4067-b881-de3bd0f176b2"
      },
      "outputs": [],
      "source": [
        "#Calculate the Missing Values in the Given Dataset before updating\n",
        "df_missing_values_before = pd.DataFrame(df1.isna().mean().round(4) * 100).reset_index()\n",
        "df_missing_values_before.columns = ['Column Name', 'Percent of Missing Values Before']\n",
        "df_missing_values_before"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Update Dataset with Missing Values\n",
        "# Fill Missing Values in \"Population\" with the sum of \"Male\" and \"Female\"\n",
        "df1['Population'] = df1['Population'].fillna(df1['Male'] + df1['Female'])\n",
        "df1['Population'] = df1['Population'].fillna(df1['Young_and_Adult'] + df1['Middle_Aged']+ df1['Senior_Citizen']+ df1['Age_Not_Stated'])\n",
        "\n",
        "#Fill Missing Values in \"Literate\" with the sum of \"Male Literate\" and \"Literate_Female\"\n",
        "df1['Literate'] = df1['Literate'].fillna(df1['Male_Literate'] + df1['Literate_Female'])\n",
        "\n",
        "#Fill Missing Values in \"SC\" with the sum of \"Male_SC\" and \"Female_SC\"\n",
        "df1['SC'] = df1['SC'].fillna(df1['Male_SC'] + df1['Female_SC'])\n",
        "\n",
        "#Fill Missing Values in \"ST\" with the sum of \"Male_ST\" and \"Female_ST\"\n",
        "df1['ST'] = df1['ST'].fillna(df1['Male_ST'] + df1['Female_ST'])\n",
        "\n",
        "#Fill Missing Values in \"Workers\" with the sum of \"Male_Workers\" and \"Female_Workers\"\n",
        "df1['Workers'] = df1['Workers'].fillna(df1['Male_Workers'] + df1['Female_Workers'])\n",
        "\n",
        "#Fill Missing Values in \"Total_Education\" with the sum of \"Below_Primary_Education\", \"Primary_Education\", \"Middle_Education\", \"Secondary_Education\",\"Higher_Education\", \"Graduate_Education\", \"Other_Education\", \"Literate_Education\"\n",
        "df1['Total_Education'] = df1['Total_Education'].fillna(df1['Below_Primary_Education']) + df1['Primary_Education'] + df1['Middle_Education'] + df1['Secondary_Education'] + df1['Higher_Education'] + df1['Graduate_Education'] + df1['Other_Education'] + df1['Literate_Education']\n",
        "\n",
        "#Fill Missing Values in \"Total_Power_Parity\" with the sum of \"Power_Parity_Less_than_Rs_45000\",\"Power_Parity_Rs_45000_90000\",\"Power_Parity_Rs_90000_150000\",\"Power_Parity_Rs_45000_150000\",\"Power_Parity_Rs_150000_240000\",\"Power_Parity_Rs_240000_330000\",\"Power_Parity_Rs_150000_330000\",\"Power_Parity_Rs_330000_425000\",\"Power_Parity_Rs_425000_545000\",\"Power_Parity_Rs_330000_545000\",\"Power_Parity_Above_Rs_545000\"\n",
        "df1['Total_Power_Parity'] = df1['Total_Power_Parity'].fillna(df1['Power_Parity_Less_than_Rs_45000'])+df1['Power_Parity_Rs_45000_90000']+df1['Power_Parity_Rs_90000_150000']+df1['Power_Parity_Rs_45000_150000']+df1['Power_Parity_Rs_150000_240000']+df1['Power_Parity_Rs_240000_330000']+df1['Power_Parity_Rs_150000_330000']+df1['Power_Parity_Rs_330000_425000']+df1['Power_Parity_Rs_425000_545000']  +df1['Power_Parity_Rs_330000_545000']+df1['Power_Parity_Above_Rs_545000']\n"
      ],
      "metadata": {
        "id": "8jmRcqbTIgEX"
      },
      "id": "8jmRcqbTIgEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the percentage of missing values after updating the missing values\n",
        "\n",
        "df_missing_values_after = pd.DataFrame(df1.isna().mean().round(4) * 100).reset_index()\n",
        "df_missing_values_after.columns = ['Column Name', 'Percent of Missing Values After']\n",
        "\n",
        "#Update the Dataframe with the Missing Values before and After\n",
        "df_missing_values = pd.merge(df_missing_values_before, df_missing_values_after, on='Column Name')\n",
        "\n",
        "\n",
        "#Calculate the difference of the percentage in the missing value before and after updating the missing value\n",
        "df_missing_values['beforeVSafter'] = df_missing_values['Percent of Missing Values Before'] - df_missing_values['Percent of Missing Values After']\n",
        "df_missing_values"
      ],
      "metadata": {
        "id": "bBkYAY8WPWz6"
      },
      "id": "bBkYAY8WPWz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MongoDB Operations"
      ],
      "metadata": {
        "id": "8pXc0iwBGZJT"
      },
      "id": "8pXc0iwBGZJT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9a78ff-5021-4d6b-92b3-bcfcf1bebdf1",
      "metadata": {
        "id": "6d9a78ff-5021-4d6b-92b3-bcfcf1bebdf1"
      },
      "outputs": [],
      "source": [
        "#Create a MongoDB Connection in TIDB and update the MongoDB Database with the DataFrame\n",
        "uri = \"mongodb+srv://csyaminisc:securetimes@cluster0.jw5gioj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(\"mongodb+srv://csyaminisc:securetimes@cluster0.jw5gioj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
        "db=client.Census #Database Name\n",
        "mycol1 = db[\"Census2011\"] #Collection Name\n",
        "fields = df1.to_dict('records')\n",
        "mycol1.insert_many(fields)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MySQL Operations"
      ],
      "metadata": {
        "id": "_d9F6NWuGxvN"
      },
      "id": "_d9F6NWuGxvN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Retrieve the Collection from MongoDB and Update MYSql\n",
        "\n",
        "# MySQL connection string\n",
        "database_username = '2Gzzgf7uUMuiWvf.root'\n",
        "database_password = 'Vz7UELZ5l4AxGUPv'\n",
        "database_ip       = 'gateway01.ap-southeast-1.prod.aws.tidbcloud.com'\n",
        "database_name     = 'Census2011'\n",
        "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
        "                                               format(database_username, database_password,\n",
        "                                                      database_ip, database_name))\n",
        "# Fetch data from MongoDB\n",
        "collection_name = 'Census2011'\n",
        "collection = db[collection_name]\n",
        "df = pd.DataFrame(list(collection.find()))\n",
        "new_column_names = {}\n",
        "for col in df.columns:\n",
        "    if len(col) > 64:\n",
        "        new_name = col[:64]  # Truncate to 64 characters (MySQL limit)\n",
        "        new_column_names[col] = new_name # Update the New Column Name\n",
        "\n",
        "# Apply the column name changes to the DataFrame\n",
        "df = df.rename(columns=new_column_names)\n",
        "\n",
        "# Convert ObjectId to string\n",
        "df['_id'] = df['_id'].astype(str) # MySQL can handle strings\n",
        "\n",
        "# Update the Dataframe to MYSQL Database\n",
        "df.to_sql(con=database_connection, name='census2011', if_exists='replace', index=False) # Don't save the index"
      ],
      "metadata": {
        "id": "9VACc8L-7DO9"
      },
      "id": "9VACc8L-7DO9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use queries.py file to run the queries in Streamlit"
      ],
      "metadata": {
        "id": "Qqj-ExyBJB0G"
      },
      "id": "Qqj-ExyBJB0G"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}